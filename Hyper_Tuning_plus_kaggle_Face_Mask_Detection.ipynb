{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyper Tuning plus kaggle 2200092 -  M4_NB_MiniProject_2_Face_Mask_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallavibekal/IISC---Neural-Networks/blob/main/Hyper_Tuning_plus_kaggle_Face_Mask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a Convolutional Neural Network model to classify between masked/unmasked/partially masked faces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812a816f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1381c49f-cd17-4bb3-e10d-7c4af5b74352"
      },
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/MP2_FaceMask_Dataset.zip\n",
        "!unzip -qq MP2_FaceMask_Dataset.zip\n",
        "print(\"Data Downloaded Successfuly!!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Downloaded Successfuly!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFcQmBZLNxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e28dafd-f5cf-4bc6-ce43-81c551920571"
      },
      "source": [
        "#!wget -qq https://www.kaggle.com/c/facemask-detection/data\n",
        "!unzip -qq /content/MP2_FaceMask_Dataset.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace MP2_FaceMask_Dataset/train/with_mask/-110603108-gettyimages-533567012.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import Required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgzMLJEBusqU",
        "outputId": "cce2fed6-9785-412d-f379-40ad84911d9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUuUuSCvu2HM"
      },
      "source": [
        "!unzip -qq /content/drive/MyDrive/FaceMask_Kaggle_test.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG52PDGENRgN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "from matplotlib import pyplot as plt\n",
        "import glob, os\n",
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "## Data Loading and preprocessing (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSjwlcSGJq1"
      },
      "source": [
        "### Analyze the shape of images and distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x42TIO3fv4oz"
      },
      "source": [
        "# Make a df for test and train"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqOBToFuv-Wz"
      },
      "source": [
        "data_dir = \"/content/MP2_FaceMask_Dataset/train\"\n",
        "test_data_dir = \"/content/MP2_FaceMask_Dataset/test\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "sePF3n5qy9M4",
        "outputId": "1b861959-b7bf-4291-a032-245a0a72ad02"
      },
      "source": [
        "images_data = glob.glob(\"/content/FullIJCNN2013/*/*.ppm\")\n",
        "len(images_data), images_data[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-379738d5509a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/FullIJCNN2013/*/*.ppm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENZ3EiAmwMVO"
      },
      "source": [
        "# Get file paths and labels\n",
        "def extract_filenames (path):\n",
        "  files = glob.glob(path + '/*')\n",
        "  actions_list = []\n",
        "  file_paths = []\n",
        "  labels_list=[]\n",
        "  for each in files:\n",
        "    actions_list.append(each.split('/')[-1])\n",
        "  for actions in actions_list:\n",
        "    filepath = path + '/' + actions + '/*'\n",
        "    filepaths = glob.glob(filepath)\n",
        "    for x in filepaths:\n",
        "      file_paths.append(x)\n",
        "      labels_list.append(actions)\n",
        "  return file_paths,labels_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqdOddVHwXbU"
      },
      "source": [
        "# Create a dataframe\n",
        "def create_df(path):\n",
        "  data_filepaths_list, labels = extract_filenames(path)\n",
        "  df_ = pd.DataFrame(list(zip(data_filepaths_list, labels)),\n",
        "                columns =['video_name', 'tag'])\n",
        "  return df_"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWgQCgfw10F"
      },
      "source": [
        "train_df = create_df(data_dir)\n",
        "test_df = create_df(test_data_dir)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfkP5DKw-MU",
        "outputId": "0c75e804-50ae-4ec6-b463-a4aac6197a34"
      },
      "source": [
        "len(train_df['video_name']), len(test_df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5056, 1263)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8x2qE3GwiSk",
        "outputId": "e2676c1e-3b9f-4ccc-ad9d-c7e13f355241"
      },
      "source": [
        "test_df['tag'].value_counts()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "without_mask    534\n",
              "with_mask       406\n",
              "partial_mask    323\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1ZYJL8Ts0Q"
      },
      "source": [
        "### Load the images using ImageDataGenerator\n",
        "\n",
        "There are two main steps involved in creating the generator.\n",
        "1. Instantiate ImageDataGenerator with required arguments to create an object\n",
        "2. Use the `flow_from_directory` command depending on how your data is stored on disk. This is the command that will allow you to generate and get access to batches of data on the fly.\n",
        "\n",
        "Hint: [link](https://keras.io/api/preprocessing/image/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqsC5w4f5NOr"
      },
      "source": [
        "TRAINING_DIR = \"/content/MP2_FaceMask_Dataset/train\"\n",
        "VALIDATION_DIR = \"/content/MP2_FaceMask_Dataset/test/\"\n",
        "KAGGLE_TEST_DIR = '/content/FaceMask_Kaggle_test/FaceMask_Kaggle_test'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzyDQ4nb6jJO",
        "outputId": "881d185e-7cc6-4c72-f9a9-b542035b8fb2"
      },
      "source": [
        "# Image Extraction using the ImageDataGenerator\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   #featurewise_center=True,\n",
        "                                   rotation_range=20,\n",
        "                                   horizontal_flip=True,\n",
        "                                   #vertical_flip=True,\n",
        "                                   brightness_range=[0.4,1.5],\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   #validation_split=0.2,\n",
        "                                   fill_mode='nearest',\n",
        "                                   )\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                 )\n",
        "                                 #horizontal_flip=True)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 interpolation=\"nearest\",\n",
        "                                                 class_mode='categorical',\n",
        "                                                 classes=['partial_mask', 'with_mask', 'without_mask'],\n",
        "                                                 #subset='training',\n",
        "                                                 seed=11,\n",
        "                                                 shuffle=True,\n",
        "                                                 #color_mode=\"grayscale\",\n",
        "                                                 batch_size = 51\n",
        "                                                 )\n",
        "                                                 #zoom_range=0.2)\n",
        "\n",
        "validation_set = val_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                 target_size=(224, 224),\n",
        "                                                 interpolation=\"nearest\",\n",
        "                                                 class_mode='categorical',\n",
        "                                                 classes=['partial_mask', 'with_mask', 'without_mask'],\n",
        "                                                 seed=11,\n",
        "                                                 shuffle=True,\n",
        "                                                 #color_mode=\"grayscale\",\n",
        "                                                 batch_size = 10)\n",
        "                                                 #subset='validation')\n",
        "\n",
        "#test_set = test_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "#                                            target_size = (224, 224),\n",
        "                                            #batch_size = 1,\n",
        "#                                            seed=11,\n",
        "#                                            class_mode=None,\n",
        "#                                            shuffle=True)\n",
        "                                            #interpolation=\"nearest\",\n",
        "                                            #color_mode='grayscale',\n",
        "#                                            class_mode='categorical',\n",
        "#                                            classes=['partial_mask', 'with_mask', 'without_mask'])\n",
        "                                            \n",
        "\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5029 images belonging to 3 classes.\n",
            "Found 1259 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQgJ-hXl4QhT",
        "outputId": "e2d2210f-5547-4e27-b8a8-dab6879e54de"
      },
      "source": [
        "next(training_set)[0].shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJbqiJaw4Vhw",
        "outputId": "989a1bc9-25dc-4b16-f484-b133dbbf053c"
      },
      "source": [
        "input_shap = next(training_set)[0].shape[1:]\n",
        "input_shap"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkOr1nhNT5yD"
      },
      "source": [
        "## Build the CNN model using Keras (4 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1GGjQTfTIoO"
      },
      "source": [
        "**Convolutional Neural Network:** A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n",
        "\n",
        "* convolutional layers\n",
        "* pooling layers\n",
        "* dense layers\n",
        "\n",
        "\n",
        "**Conv2D**  \n",
        "\n",
        "Passing an image with input shape of 3-D and to calculate the output: \n",
        "\n",
        " $O = \\frac{n - f + 2p}{s} + 1$\n",
        "\n",
        "**MaxPool** \n",
        "\n",
        "The resulting output, when using the \"valid\" padding option, has a spatial shape (number of rows or columns) of: \n",
        "\n",
        "O = `math.floor`$(\\frac{input shape - pool size)}{ strides}) + 1$ (when input shape >= pool size)\n",
        "\n",
        "The resulting output shape when using the \"same\" padding option is: \n",
        "\n",
        "O = `math.floor`$(\\frac{input shape - 1}{strides}) + 1$\n",
        "\n",
        "by default, stride = None, so stride is same as pool size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGNu31vmLhs"
      },
      "source": [
        "Task-flow\n",
        "* Initialize the network of convolution, maxpooling and dense layers\n",
        "* Define the optimizer and loss functions\n",
        "* Fit the model and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJrf-VuhLUGN"
      },
      "source": [
        "model = keras.Sequential([\n",
        "#  keras.layers.Rescaling(1./255),\n",
        "  keras.layers.Conv2D(256, kernel_size=(4,4), strides=(3,3), activation='relu',padding=\"same\", input_shape=[224, 224, 3]),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1)),\n",
        "  keras.layers.BatchNormalization(),\n",
        "  #keras.layers.Conv2D(128, kernel_size=(7,7), strides=(3,3), activation='relu',padding=\"same\"),\n",
        "  #keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "  #keras.layers.BatchNormalization(),\n",
        "  keras.layers.Conv2D(384, kernel_size=(5,5), strides=(2,2), activation='relu',padding=\"same\"),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1)),\n",
        "  keras.layers.BatchNormalization(),\n",
        "  keras.layers.Conv2D(384, kernel_size=(3,3), strides=(2,2), activation='relu',padding=\"same\"),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "  keras.layers.BatchNormalization(),\n",
        "  keras.layers.Dropout(0.5),\n",
        "  keras.layers.Flatten(),\n",
        "  #keras.layers.Dense(512, activation='relu'),\n",
        "  keras.layers.Dense(192, activation='relu'),\n",
        "  keras.layers.Dense(96, activation='relu'),\n",
        "  keras.layers.Dropout(0.5),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(3, activation = 'softmax')\n",
        "])\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZQyDx9PemMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc708c1d-f23c-42e0-e2f7-96b22ee7e379"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 75, 75, 256)       4352      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 74, 74, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 74, 74, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 37, 37, 384)       2457984   \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 36, 36, 384)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 36, 36, 384)      1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 18, 18, 384)       1327488   \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 9, 9, 384)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 9, 9, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 9, 9, 384)         0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 31104)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 192)               5972160   \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 96)                18528     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 96)                0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 291       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,784,899\n",
            "Trainable params: 9,782,851\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgR8dkb_x1wL"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "optimizer = Adam(learning_rate=0.0001)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPqPtaURy2SQ"
      },
      "source": [
        "callbacks = [ keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\")]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX5mpW9TOqr_"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aePfroiE6n-",
        "outputId": "47e53079-6536-4605-b934-bbaf2da8bc30"
      },
      "source": [
        "history = model.fit(training_set, validation_data=validation_set ,epochs = 25,callbacks=callbacks, class_weight={0:1.0,1:0.8,2:1.2}) #validation_data=test_ds)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "503/503 [==============================] - 112s 222ms/step - loss: 1.2303 - accuracy: 0.5826 - val_loss: 0.5211 - val_accuracy: 0.7998\n",
            "Epoch 2/25\n",
            "503/503 [==============================] - 110s 220ms/step - loss: 0.7945 - accuracy: 0.7021 - val_loss: 0.3674 - val_accuracy: 0.8562\n",
            "Epoch 3/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.6341 - accuracy: 0.7566 - val_loss: 0.2936 - val_accuracy: 0.8999\n",
            "Epoch 4/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.5715 - accuracy: 0.7841 - val_loss: 0.2909 - val_accuracy: 0.9031\n",
            "Epoch 5/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.4743 - accuracy: 0.8127 - val_loss: 0.2496 - val_accuracy: 0.9317\n",
            "Epoch 6/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.4581 - accuracy: 0.8200 - val_loss: 0.2565 - val_accuracy: 0.9325\n",
            "Epoch 7/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.4037 - accuracy: 0.8485 - val_loss: 0.2161 - val_accuracy: 0.9412\n",
            "Epoch 8/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.3800 - accuracy: 0.8536 - val_loss: 0.1679 - val_accuracy: 0.9547\n",
            "Epoch 9/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.3464 - accuracy: 0.8664 - val_loss: 0.1612 - val_accuracy: 0.9571\n",
            "Epoch 10/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.3121 - accuracy: 0.8789 - val_loss: 0.1475 - val_accuracy: 0.9484\n",
            "Epoch 11/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.2879 - accuracy: 0.8912 - val_loss: 0.1416 - val_accuracy: 0.9579\n",
            "Epoch 12/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.2763 - accuracy: 0.8948 - val_loss: 0.1318 - val_accuracy: 0.9658\n",
            "Epoch 13/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.2571 - accuracy: 0.9067 - val_loss: 0.1239 - val_accuracy: 0.9666\n",
            "Epoch 14/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.2547 - accuracy: 0.9071 - val_loss: 0.1250 - val_accuracy: 0.9651\n",
            "Epoch 15/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.2379 - accuracy: 0.9119 - val_loss: 0.1233 - val_accuracy: 0.9595\n",
            "Epoch 16/25\n",
            "503/503 [==============================] - 109s 217ms/step - loss: 0.2346 - accuracy: 0.9113 - val_loss: 0.1339 - val_accuracy: 0.9619\n",
            "Epoch 17/25\n",
            "503/503 [==============================] - 109s 218ms/step - loss: 0.2233 - accuracy: 0.9183 - val_loss: 0.0973 - val_accuracy: 0.9658\n",
            "Epoch 18/25\n",
            "503/503 [==============================] - 109s 218ms/step - loss: 0.1989 - accuracy: 0.9252 - val_loss: 0.0984 - val_accuracy: 0.9722\n",
            "Epoch 19/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.1937 - accuracy: 0.9250 - val_loss: 0.0839 - val_accuracy: 0.9738\n",
            "Epoch 20/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.1866 - accuracy: 0.9284 - val_loss: 0.1212 - val_accuracy: 0.9658\n",
            "Epoch 21/25\n",
            "503/503 [==============================] - 112s 224ms/step - loss: 0.1846 - accuracy: 0.9334 - val_loss: 0.0992 - val_accuracy: 0.9714\n",
            "Epoch 22/25\n",
            "503/503 [==============================] - 109s 217ms/step - loss: 0.1759 - accuracy: 0.9354 - val_loss: 0.1104 - val_accuracy: 0.9674\n",
            "Epoch 23/25\n",
            "503/503 [==============================] - 110s 219ms/step - loss: 0.1781 - accuracy: 0.9344 - val_loss: 0.0912 - val_accuracy: 0.9722\n",
            "Epoch 24/25\n",
            "503/503 [==============================] - 110s 218ms/step - loss: 0.1715 - accuracy: 0.9360 - val_loss: 0.0697 - val_accuracy: 0.9786\n",
            "Epoch 25/25\n",
            "503/503 [==============================] - 109s 216ms/step - loss: 0.1634 - accuracy: 0.9372 - val_loss: 0.0813 - val_accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQjuKmry5mW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd9b6d1-dc41-4f3e-f30e-b2eae2567268"
      },
      "source": [
        "model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "#test_loss, test_acc = model.evaluate(test_set)\n",
        "#print(\"Test loss\", test_loss)\n",
        "#print(\"Test accuracy\", test_acc)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPKELTRnv9s"
      },
      "source": [
        "## Transfer learning (4 points)\n",
        "\n",
        "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n",
        "\n",
        "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task.\n",
        "\n",
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
        "\n",
        "For eg. Using VGG16, we remove the last layer which takes a probability for each of the 1000 classes in the ImageNet and replaces it with a layer that takes 3 probabilities in our case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lMUVOj0UdAi"
      },
      "source": [
        "### Use the pre-trained models ([VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16) or [ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50))\n",
        "\n",
        "* Load the pre-trained model\n",
        "* Fit and evaluate the data\n",
        "\n",
        "Hint: [How to use pre-trained model](https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YI0gnq-DLyu"
      },
      "source": [
        "#### Expected accuracy: More than 90%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4GX9CwFqOXm"
      },
      "source": [
        "Task-flow\n",
        "* Initialize the network with the weights of Imagenet\n",
        "* Fine tune the network by modifying fully connected layers.\n",
        "* Re-train the model with our problem data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx2Z2vLZqXm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f050a2-1a63-4202-d94e-414551cf4e45"
      },
      "source": [
        "model_vgg = Sequential()\n",
        "model_vgg.add(VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3)))\n",
        "model_vgg.add(Flatten())\n",
        "model_vgg.add(Dense(512,activation=\"relu\",kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.02)))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(Dropout(0.5))\n",
        "model_vgg.add(Dense(96,activation=\"relu\",kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.02)))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(Dropout(0.5))\n",
        "model_vgg.add(Dense(training_set.num_classes,activation='softmax'))\n",
        "model_vgg.summary()\n",
        "\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 96)                49248     \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 96)               384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 96)                0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 3)                 291       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,612,227\n",
            "Trainable params: 27,611,011\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2reK-xjI9ps"
      },
      "source": [
        "for idx in range(len(model_vgg.layers)-1): #Leave the second to last layer trainable\n",
        "  model.layers[idx].trainable = False\n",
        "model_vgg.layers[len(model_vgg.layers)-1].trainable = True "
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVV-8meNEhqL"
      },
      "source": [
        "optimizer_adam = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "model_vgg.compile(optimizer=optimizer_adam, loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k63DJNTxpvD8"
      },
      "source": [
        "from sklearn.utils import compute_class_weight"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zimeh1OQmIDe"
      },
      "source": [
        "class_weight = compute_class_weight(class_weight='balanced',\n",
        "                                        classes=np.unique(train_df['tag']),\n",
        "                                        y=train_df['tag'])\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqI3Tl9bpwve",
        "outputId": "b7a15e91-7d5c-4ba5-8a5d-4eb79097d236"
      },
      "source": [
        "class_weight"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.30242143, 1.03776683, 0.78827565])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG0r1cH-wZYC"
      },
      "source": [
        "callbacks = [ keras.callbacks.ModelCheckpoint(\"best_model_vgg1.h5\", save_best_only=True, monitor=\"val_loss\")]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpI8BCfEGYvu",
        "outputId": "36b5d625-59ac-4e85-ae24-8fd0887e1948"
      },
      "source": [
        "history3 = model_vgg.fit(training_set,epochs=10,\n",
        "                              validation_data=validation_set, class_weight={0:1.3,1:1,2:0.7},callbacks=callbacks)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.3823 - acc: 0.9994 - val_loss: 0.3965 - val_acc: 0.9952\n",
            "Epoch 2/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.3713 - acc: 0.9982 - val_loss: 0.3752 - val_acc: 0.9968\n",
            "Epoch 3/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.3576 - acc: 0.9986 - val_loss: 0.3614 - val_acc: 0.9968\n",
            "Epoch 4/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.3458 - acc: 0.9992 - val_loss: 0.3780 - val_acc: 0.9921\n",
            "Epoch 5/10\n",
            "99/99 [==============================] - 112s 1s/step - loss: 0.3350 - acc: 0.9984 - val_loss: 0.3364 - val_acc: 0.9968\n",
            "Epoch 6/10\n",
            "99/99 [==============================] - 112s 1s/step - loss: 0.3229 - acc: 0.9984 - val_loss: 0.3182 - val_acc: 0.9984\n",
            "Epoch 7/10\n",
            "99/99 [==============================] - 111s 1s/step - loss: 0.3101 - acc: 0.9990 - val_loss: 0.3195 - val_acc: 0.9944\n",
            "Epoch 8/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.2977 - acc: 0.9996 - val_loss: 0.3003 - val_acc: 0.9960\n",
            "Epoch 9/10\n",
            "99/99 [==============================] - 112s 1s/step - loss: 0.2899 - acc: 0.9988 - val_loss: 0.3078 - val_acc: 0.9929\n",
            "Epoch 10/10\n",
            "99/99 [==============================] - 113s 1s/step - loss: 0.2785 - acc: 0.9992 - val_loss: 0.2937 - val_acc: 0.9968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4avZRiuot9g"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['img_path'] = glob.glob(\"FaceMask_Kaggle_test/*\")\n",
        "df['order'] = [int(i.split(\"/\")[1][:-4]) for i in df['img_path']]\n",
        "df.sort_values('order',inplace=True)\n",
        "df.reset_index(inplace=True,drop=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M5bsxjmoxCA"
      },
      "source": [
        "kaggle_features = []\n",
        "for i in df.img_path:\n",
        "\n",
        "#  im = np.array(PIL.Image.open(i).convert('L').resize((224,224)))\n",
        "  im = np.array(PIL.Image.open(i)  .resize((224,224)))\n",
        "  if im.shape != (224,224,3):\n",
        "    print(i, im.shape)\n",
        "  kaggle_features.append(im)\n",
        "  \n",
        "kaggle_features = np.array(kaggle_features)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HevvYKMao3FY"
      },
      "source": [
        "# Loading the saved model\n",
        "model = keras.models.load_model('/content/best_model_vgg1.h5')"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxWXSjAgrrId"
      },
      "source": [
        "model = keras.models.load_model('/content/best_model.h5')"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLvCr3K9pQlZ"
      },
      "source": [
        "pred = model.predict(kaggle_features)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQPZYO9so56N",
        "outputId": "434eea33-b5ac-4de5-c99a-5b92e4c8093f"
      },
      "source": [
        "act_pred = np.argmax(pred,axis=1)\n",
        "set(act_pred)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2}"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGJj4Bqso8Fd"
      },
      "source": [
        "kaggle = pd.DataFrame()\n",
        "kaggle['label'] = act_pred\n",
        "kaggle['img_path'] = df['img_path']\n",
        "kaggle['label'].replace(0,'partial_mask',inplace=True)\n",
        "kaggle['label'].replace(1,'with_mask',inplace=True)\n",
        "kaggle['label'].replace(2,'without_mask',inplace=True)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXjIYMNmo-bE"
      },
      "source": [
        "kaggle.to_csv(\"kaggle_submission_w13.csv\",index=False)"
      ],
      "execution_count": 159,
      "outputs": []
    }
  ]
}